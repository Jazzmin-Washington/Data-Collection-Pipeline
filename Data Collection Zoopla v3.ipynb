#%%
# Import the necessary
import selenium
from selenium.webdriver import Chrome
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from bs4 import BeautifulSoup
import time
#%% 
# Initalise Scraper Class
class Scraper():
    def __init__(self):
        self.driver = Chrome('./chromedriver')
        url = 'https://www.zoopla.co.uk/to-rent/property/liverpool/?beds_min=1&page_size=25&price_frequency=per_month&price_max=700&view_type=list&q=Liverpool%2C%20Merseyside&radius=0&results_sort=newest_listings&search_source=refine'
        # Scraper Class to Navigate Website
        self.driver.get(url)
        self.full_prop_list = []
        self.data_list = []
        self.link_list = []
        self.delay = 10

#%%
# Load and Accept Cookies
    def load_and_accept_cookies(self):
            time.sleep(10)
            try:
                self.driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame
                accept_cookies_button = WebDriverWait(self.driver, self.delay).until(EC.presence_of_element_located((By.XPATH, '//*[@id="save"]')))
                accept_cookies_button.click()
                time.sleep(1)
            except AttributeError: # If you have the latest version of selenium, the code above won't run because the "switch_to_frame" is deprecated
                self.driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame
                accept_cookies_button = self.driver.find_element_by_xpath('//*[@id="save"]')
                accept_cookies_button.click()
                time.sleep(1)

            except:
                pass
#%% 
# Get the links

    def get_links(self):
        self.prop_container = self.driver.find_element_by_xpath('//div[@class="css-1anhqz4-ListingsContainer e1b8efd72"]')
        self.prop_list = self.prop_container.find_elements_by_xpath('./div')
        time.sleep(10)
        for property in self.prop_list:
            a_tag = property.find_element_by_tag_name('a')
            link = a_tag.get_attribute('href')
            self.link_list.append(link)

        print(f'There are {len(self.link_list)} properties in this page')
        print(self.link_list)
        self.next_page()
#%% 
# Go to the next page
    def next_page(self):
            time.sleep(10)
            pages = 0
            if pages > 4:
                self.full_prop_list.extend(self.get_links())
                next_page_button = self.driver.find_element_by_xpath('//a[@class="eaoxhri5 css-xtzp5a-ButtonLink-Button-StyledPaginationLink eaqu47p1"]')
                next_page_button.click()
                pages = pages + 1
                time.sleep(5)
            else:
                self.get_data()
#%%
# Get the data from the website
    def get_data(self):
        self.num_prop = len(self.full_prop_list)
        for i in range(self.num_props):
            self.driver.get(self.full_prop_list[i])
            time.sleep(25)
            self.data = {i: {"rent_price": [], "weekly_price": [],"bed_and_bath_num": [], "description": [], "address": []}}
            time.sleep(15)
            # Try to find each corresponding information and append 'None' if not present
            try:
                rent_price = self.driver.find_element_by_xpath('//p[@data-testid = "price"]').text
                self.data[i]['rent_price'].append(rent_price)
            except NoSuchElementException:
                self.data[i]['rent_price'].append('None')

            try:
                weekly_price = self.driver.find_element_by_xpath('//p[@data-testid="rentalfrequency-and-floorareaunit"]').text
                self.data[i]['weekly_price'].append(weekly_price)
            except NoSuchElementException:
                self.data[i]['weekly_price'].append('None')

            try:
                bed_and_bath_num = self.driver.find_element_by_xpath('//div[@class="c-PJLV c-PJLV-iiNveLf-css"]').text
                self.data[i]['bed_and_bath_num'].append(bed_and_bath_num)
            except NoSuchElementException:
                self.data[i]['bed_and_bath_num'].append('None')

                
            try:
                description = self.driver.find_element_by_xpath('//div[@data-testid="listing_description"]').text
                self.data[i]['description'].append(description)
            except NoSuchElementException:
                self.data[i]['description'].append('None')

            try:
                address = self.driver.find_element_by_xpath('//address[@ data-testid="address-label"]').text
                self.data[i]['address'].append(address)
            except NoSuchElementException:
                self.data[i]['address'].append('None')

            self.full_data_list.append(self.data)
            print(self.full_data_list[i])
            time.sleep(10)
           
#%%
# Run functions
    def scrape_website(self):
        self.load_and_accept_cookies()
        self.get_links()
        self.get_data()
        self.driver.quit()

new_Scraper_Website = Scraper()
new_Scraper_Website.scrape_website()