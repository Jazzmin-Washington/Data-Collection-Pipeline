#%%
# 
import selenium
from selenium.webdriver import Chrome
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from bs4 import BeautifulSoup
import time

class Scraper():

    def __init__(self):
        self.driver = Chrome('./chromedriver')
        url = 'https://www.zoopla.co.uk/to-rent/property/liverpool/?beds_min=1&page_size=25&price_frequency=per_month&price_max=700&view_type=list&q=Liverpool%2C%20Merseyside&radius=0&results_sort=newest_listings&search_source=refine'
        # Scraper Class to Navigate Website
        self.driver.get(url)
        self.full_prop_list = []
        self.data_list = []
        self.link_list = []
        self.delay = 10
        self.pages = 0


# Look for cookies button and accept cookies
    def load_and_accept_cookies(self):
        time.sleep(10)
        try:
            self.driver.switch_to.frame('gdpr-consent-notice') # This is the id of the frame
            accept_cookies_button = WebDriverWait(self.driver, self.delay).until(EC.presence_of_element_located((By.XPATH, '//*[@id="save"]')))
            accept_cookies_button.click()
            time.sleep(1)
        except AttributeError: # If you have the latest version of selenium, the code above won't run because the "switch_to_frame" is deprecated
            self.driver.switch_to_frame('gdpr-consent-notice') # This is the id of the frame
            accept_cookies_button = WebDriverWait(self.driver, self.delay).until(EC.presence_of_element_located((By.XPATH, '//*[@id="save"]')))
            accept_cookies_button.click()
            time.sleep(1)

        except:
            pass

# Get links for each of the entries

    def get_links(self):
        self.prop_container = WebDriverWait(self.driver, self.delay).until(EC.presence_of_element_located((By.XPATH,'//div[@class="css-1anhqz4-ListingsContainer e1b8efd72"]')))
        self.prop_list = self.prop_container.find_elements_by_xpath('./div')
        time.sleep(10)
        for property in self.prop_list:
            a_tag = property.find_element_by_tag_name('a')
            link = a_tag.get_attribute('href')
            self.link_list.append(link)

        print(f'There are {len(self.link_list)} properties in this page')
        print(self.link_list)
        self.next_page()

 # Go to the next page   
    def next_page(self):
        time.sleep(15)
        if self.pages == 0:
            next_page_button = self.driver.find_element_by_xpath('//li[@class="css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2"]')
            next_page_button.click()
            self.pages += 1
            print(self.pages)
            time.sleep(15)
            cancel_button = self.driver.find_element_by_xpath('//button[@data-testid="modal-close"]')
            cancel_button.click()
            self.get_links()
          
        
        elif self.pages == 4:
            self.get_data()

        else:
            time.sleep(15)
            next_page = self.driver.find_element_by_xpath('//li[@class="css-qhg1xn-PaginationItemPreviousAndNext-PaginationItemNext eaoxhri2"]')
            next_page.click()
            self.pages += 1
            print(self.pages)
            time.sleep(15)
            self.get_links()
           
       

 # Get the necessary bdata from the website   
    def get_data(self):
        self.num_props = len(self.link_list)
        print(self.num_props)
        for i in range(self.num_props):
            self.driver.get(self.link_list[i])
            time.sleep(15)
            self.data = {i: {"rent_price": [], "weekly_price": [],"bed_and_bath_num": [], "description": [], "address": []}}
            time.sleep(15)
            # Try to find each corresponding information and append 'None' if not present
            try:
                rent_price = self.driver.find_element_by_xpath('//p[@data-testid = "price"]').text
                self.data[i]['rent_price'].append(rent_price)
            except NoSuchElementException:
                self.data[i]['rent_price'].append('None')

            try:
                weekly_price = self.driver.find_element_by_xpath('//p[@data-testid="rentalfrequency-and-floorareaunit"]').text
                self.data[i]['weekly_price'].append(weekly_price)
            except NoSuchElementException:
                self.data[i]['weekly_price'].append('None')

            try:
                bed_and_bath_num = self.driver.find_element_by_xpath('//div[@class="c-PJLV c-PJLV-iiNveLf-css"]').text
                self.data[i]['bed_and_bath_num'].append(bed_and_bath_num)
            except NoSuchElementException:
                self.data[i]['bed_and_bath_num'].append('None')

                
            try:
                description = self.driver.find_element_by_xpath('//div[@data-testid="listing_description"]').text
                self.data[i]['description'].append(description)
            except NoSuchElementException:
                self.data[i]['description'].append('None')

            try:
                address = self.driver.find_element_by_xpath('//address[@ data-testid="address-label"]').text
                self.data[i]['address'].append(address)
            except NoSuchElementException:
                self.data[i]['address'].append('None')

            self.data_list.append(self.data)
            print(self.data_list[i])
            time.sleep(10)
        
           
# Run the programs and functions
    def scrape_website(self):
        self.load_and_accept_cookies()
        self.get_links()
        self.driver.quit()

new_Scraper_Website = Scraper()
new_Scraper_Website.scrape_website()
# %%
